---
layout: chapter
title: "신경망 학습 방법"
includes: [mathjax]
header_image: "/images/headers/topographic_map.jpg"
header_text: "A <a href=\"http://www.summitpost.org/ruth-creek-topographic-map/771858\">topographic map</a> depicts elevation with contour lines connecting places at equal heights."

---

<!--

Gradient descent isn't the only way to solve neural networks. Notably, BGFS (or LBGFS when memory is limited) is sometimes used, but it operates on a similar principle: iterative, small weight updates convering on a good solution. 

todo/more sections?
 - LBGFS, Adam
 - Batchnorm
 - preprocessing (norm, standard), weight init
 - choice of loss function (categorical cross-entropy)
 - use L or C instead of J

-->

[日本語](/ml4a/jp/how_neural_networks_are_trained/)

본인이 산꼭대기에 있는 등산가이고 밤이 깊었다고 상상해 봅시다. 산 아래 베이스캠프에 도착해야 하는데, 희미한 손전등만 있는 이 어둠 속에서는 몇 피트 정도의 땅밖에 보이지 않습니다. 어떻게 내려갈 수 있을까요? 하나의 방법으로는 어느 쪽으로 가장 땅이 기울고 있는지 확인한 다음 그 방향으로 나아가는 것이 있겠습니다. 이 과정을 여러 번 반복하면 내리막길을 따라 점점 아래로 내려갈 수 있습니다. 때때로 작은 골이나 계곡에 갇히게 될 수도 있는데, 이 경우 움직인 방향으로 조금 더 많이 움직여서 계곡을 벗어날 수도 있습니다. 이러한 주의사항만 있으면 이 전략으로 결국 하산할 수 있게 됩니다.

신경망과 전혀 관계가 없는 것처럼 보일 수 있지만, 이 이야기는 신경망이 학습되는 방식에 대한 좋은 비유입니다. 사실, 그렇게 하기 위한 기본적인 기술인 [경사 하강법](https://ko.wikipedia.org/wiki/%EA%B2%BD%EC%82%AC_%ED%95%98%EA%B0%95%EB%B2%95)은 우리가 방금 설명한 것과 매우 흡사합니다. 학습은 신경망의 정확도를 최대화하기 위한 최상의 가중치 세트를 결정하는 것을 말합니다. 앞 장에서는 이 과정을 블랙박스 안에 넣어 둔 채로 이미 학습된 네트워크가 무엇을 할 수 있는지 살펴보았습니다. 그러나 이 장에서는 경사 하강법이 어떻게 작용하는지에 대한 세부 사항을 설명할 것이며, 그것이 방금 설명한 등반가 비유와 아주 유사하다는 것을 알게 될 것입니다.

신경망은 안에 있는 전자제품이 어떻게 작동하는지 모른 채 손전등을 조작할 수 있는 것처럼 학습 과정을 정확히 알지 못한 채 사용할 수 있습니다. 대부분의 현대 기계 학습 라이브러리는 학습 과정을 굉장히 자동화했습니다. 자동화된 라이브러리에 의지할 수 있고, 이 주제가 수학적으로 더 어렵기 때문에, 그것을 따로 두고 신경망의 응용 분야로 달려가고 싶은 유혹을 느낄지도 모릅니다. 그러나 그 과정을 이해하는 것은 신경망이 어떻게 적용되고 재구성될 수 있는지에 대한 귀중한 통찰력을 제공하기 때문에, 용감한 독자는 그 과정을 이해하지 않는 것이 실수라는 것을 알고 있을 것입니다. 게다가, 대형 신경망 학습 능력은 수년 동안 이해할 수 없었고 최근에서야 실현 가능해져, 현재 가장 활동적이고 흥미로운 연구 분야 중 하나일 뿐만 아니라 인공지능 역사상 가장 위대한 성공 사례 중 하나가 되었습니다.

이 장의 목적은 신경망의 해결 방법에 대한 엄격한 이해는 아닐지라도 직관적인 이해를 제공하는 것입니다. 가능한 한 방정식보다 그림으로 설명될 것이며, 추가적인 판독과 정교함을 위한 외부 링크가 제공될 것입니다. 우리는 경사 하강법, 역전파, 그리고 몇 가지 부분에서 관련된 모든 기술을 이야기 할 것입니다. 하지만 우선, 왜 학습이 어려운지 이해하는 것부터 시작합시다.

# 학습은 왜 어려운가

## 초-차원(hyper-dimensional) 건초 더미 안의 바늘

숨겨진 레이어가 있는 신경망의 가중치는 상호의존성이 아주 높습니다. 그 이유를 알기 위해, 아래 세 계층 네트워크의 첫 번째 계층에서 강조 표시된 연결을 고려해봅시다. 만약 우리가 그 연결에서 가중치를 약간 조정한다면, 그 연결이 직접적으로 전파하는 뉴런뿐만 아니라 다음 두 층의 모든 뉴런에도 영향을 미칠 것이고, 따라서 _모든_ 출력에도 영향을 미칠 것입니다.

{% include figure_multi.md path1="/images/figures/connection_tweak.png" caption1="첫 번째 층에서 한 연결의 가중치를 조절하는 것은 다음 층 한 개의 뉴런에만 영향을 미치지만, 완전히 연결되어 있어서 이후 다음 층의 모든 뉴런은 바뀔 것이다." %}

이러한 이유로, 우리는 한 번에 하나씩 최적화함으로써 최고의 가중치 세트를 얻을 수 없다는 것을 알 수 있습니다. 우리는 전체 가중치 조합을 가능한 동시에 찾아야 합니다. 우리가 이걸 어떻게 할 수 있을까요?

가장 단순하고 가장 쉬운 접근법부터 시작해보죠, 무작위 추측입니다. 네트워크의 모든 가중치를 랜덤 값으로 설정하고 데이터 세트에서 정확성을 평가하는 방법입니다. 결과를 추적하면서 이것을 여러 번 반복하고 우리에게 가장 정확한 결과를 준 일련의 가중치를 저장합니다. 처음에 이것은 합리적인 접근으로 보일 수 있습니다. 결국, 컴퓨터는 매우 빠르므로 아마도 우리는 이 무작위 대입 방법으로 괜찮은 해결책을 얻을 수 있을 것입니다. 수십 개의 뉴런이 있는 네트워크라면, 이 방법이 효과가 있을 겁니다. 우리는 빠르게 수백만 가지 추측을 할 수 있고 그것들로부터 괜찮은 후보자를 얻을 수 있을 것입니다. 하지만 대부분의 실제 응용 프로그램에서는 그보다 훨씬 더 많은 가중치를 가집니다. [이전 장](/ml4a/ko/neural_networks/)의 필기 예제를 살펴 봅시다. 약 12,000개의 가중치 값이 있습니다. 그 많은 것들 중에서 가장 좋은 무게의 조합은 말 그대로 건초 더미 안의 바늘과 같습니다. 이 건초 더미는 12,000개의 차원을 가지고 있다는 것을 제외하면요!

여러분은 12,000차원 건초 더미가 더 친숙한 3차원 건초 더미보다 "4,000배밖에 안 된다"고 생각할지도 모릅니다. 그래서 최고의 가중치를 찾기 위해서는 4,000배의 시간이 필요하다고 말입니다. 하지만 실제로는 그 비율이 이해할 수 없을 정도로 큽니다. 그리고 다음 섹션에서 그 이유를 알아보겠습니다.

## 외로운 n차원 공간

우리의 전략이 무차별적인 무작위 검색이라면, 합리적으로 좋은 가중치 세트를 얻기 전에 얼마나 많은 추측을 해야 할지 생각해볼 수 있습니다. 직관적으로, 가능한 추측의 전체 공간(space)을 촘촘히 표본으로 추출하기 위해 충분한 추측을 해야 한다고 예상할 수 있습니다. 사전 지식이 없으면 올바른 가중치가 어디에든 숨겨질 수 있기 때문에 가능한 모든 공간을 표본으로 추출하는 것이 타당합니다.

이것을 설명하기 위해, 두 개의 매우 작은 1층 신경망을 생각해 봅시다. 첫 번째는 2개의 신경망을 가지고 있고, 두 번째는 3개의 신경망을 가지고 있습니다. 단순하게 보기 위해 당분간 편향(bias)은 무시합시다.

{% include figure_multi.md path1="/images/figures/small_nets.png" caption1="각각 두 개의 가중치 연결과 세 개의 가중치 연결을 가진 두 개의 작은 네트워크(당분간 편향은 무시함)." %}

첫 번째 네트워크에는 두 가지 가중치가 있습니다. 그중 하나가 잘 맞을 거라고 확신하려면 얼마나 많은 추측을 해야 할까요? 이 질문에 접근하는 한 가지 방법은 가능한 체중 조합의 2차원 공간을 상상하고 모든 조합을 어느 정도 세분화 수준까지 철저히 검색하는 것입니다. 아마 우리는 각 축을 10개의 세그먼트로 나눌 수 있습니다. 그러면 우리의 추측은 이 둘의 모든 조합일 것입니다; 모두 100개입니다. 그렇게 나쁘지는 않습니다. 이러한 밀도의 표본 추출은 대부분의 공간을 상당히 잘 고려할 수 있습니다. 축을 10개가 아닌 100개의 세그먼트로 나누면 100*100=10,000개의 추측을 만들어 공간을 매우 촘촘하게 나눠서 고려해야 합니다. 10,000개의 추측은 여전히 매우 작습니다. 어떤 컴퓨터도 1초 안에 계산할 수 있습니다.

두 번째 네트워크는 어떨까요? 여기 두 개 대신 세 개의 가중치가 있고, 따라서 3차원 공간을 탐색해야 합니다. 이 공간을 2차원 네트워크를 샘플링한 것과 동일한 수준으로 샘플링하려면 각 축을 다시 10개의 세그먼트로 나눕니다. 이제 10 * 10 * 10 = 1,000개의 추측이 있습니다. 2차원과 3차원 시나리오는 모두 아래 그림에 설명되어 있습니다.

{% include figure_multi.md path1="/images/figures/sampling.png" caption1="왼쪽: 10% 밀도로 표본 추출된 2차원 정사각형에는 10² = 100개의 점이 필요합니다. 오른쪽: 10% 밀도로 표본 추출된 3차원 정육면체에는 10³ = 1,000개의 점이 필요합니다." %}

1,000개의 추측은 식은 죽 먹기라고 말할 수 있습니다. 100개의 세그먼트로 세분화하면 $$100 * 100 * 100 = 1000000$$ 추측이 있을 것입니다. 1,000,000개의 추측은 여전히 문제가 되지 않지만, 슬슬 조금씩 긴장이 됩니다. 이 접근 방식을 보다 현실적인 규모의 네트워크로 확장하면 어떻게 됩니까? 가능한 추측의 수는 우리가 가진 가중치의 수와 관련하여 기하급수적으로 증가한다는 것을 알 수 있습니다. 일반적으로, 한 축당 10개의 세그먼트로 세분화하여 샘플링하려면 $$N$$차원 데이터 세트에 $$10^N$$ 샘플이 필요합니다.

그렇다면 이 방법을 사용하여 [첫 번째 장](/ml4a/ko/neural_network/)의 MNIST 숫자를 분류하기 위한 네트워크를 학습시키려면 어떻게 해야 할까요? 네트워크가 784개의 입력 뉴런, 즉, 15개의 뉴런이 1개의 숨겨진 층에 있고 10개의 뉴런이 출력층에 있었습니다. 따라서 $$784*15 + 15*10 = 11910$$의 가중치가 있습니다. 여기에 25개의 편향까지 11,935개의 매개 변수를 통해 동시에 추측해야 합니다. $$10^{11935}$$의 추측을 해야 한다는 뜻이죠. 거의 12,000개의 0이 있는 1입니다! 상상할 수 없을 정도로 큰 숫자입니다. 쉽게 비교하자면, 전 우주에는 $$10^{80}$$개의 원자가 있습니다. 어떤 슈퍼컴퓨터도 그렇게 많은 계산을 할 수는 없습니다. 사실, 만약 오늘날 세상에 존재하는 모든 컴퓨터들을 지구와 태양에 충돌할 때까지 작동시킨다 하더라도, 여전히 계산 중일 것입니다! 현대의 심층 신경망은 수천, 수억 개의 가중치를 가지고 있다는 것을 생각해보세요.

이 원리는 우리가 기계 학습에서 "[차원의 저주](https://en.wikipedia.org/wiki/Curse_of_dimensionality)"라고 부르는 것과 밀접하게 관련되어 있습니다. 검색 공간에 추가되는 각 차원은 학습된 모델의 우수한 일반화를 위해 필요한 샘플 수를 기하급수적으로 증가시킵니다. 차원의 저주는 데이터 세트에 적용되는 경우가 더 많습니다. 간단히 말해 데이터 세트가 더 많은 열 또는 변수로 표현될수록 해당 데이터 세트에서 더 많은 샘플을 고려해야 한다는 것입니다. 우리는 입력보다는 가중치에 대해 생각하고 있지만, 원칙은 같습니다; [고차원 공간은 어마어마하다](https://www.inf.fu-berlin.de/inst/ag-ki/rojas_home/documents/tutorials/dimensionality.pdf)!

분명히 이 문제에는 무작위적인 추측보다 더 우아한 해결책이 필요합니다. 이러한 문제를 해결하기 위한 효율적인 계산 방법에 대한 이해를 높이기 위해, 신경망을 잠시 잊고 간단한 문제로 시작하여 경사 하강법에 도달할 때까지 점진적으로 나아가봅시다.

# 선형 회귀

[선형 회귀](https://ko.wikipedia.org/wiki/선형_회귀)는 일련의 데이터 포인트를 통해 "가장 적합한 선"을 결정하는 작업을 말하며 신경망 해결에 사용하는 보다 복잡한 비선형 방법에 선행하는 단순한 방법입니다. 이 절에서는 선형 회귀 분석의 예를 보여 줍니다. 아래 그림의 왼쪽 표와 같이 7개의 점으로 이루어진 집합이 있다고 가정해봅시다. 오른쪽에는 이 점들의 산점도가 있습니다.

{::nomarkdown}
<div style="text-align:center;">
	<div style="display:inline-block; vertical-align:middle; margin-right:100px;">
		<table width="200" style="border: 1px solid black;">
		  	<tbody>
				<tr>
					<td><script type="math/tex">x</script></td>
					<td><script type="math/tex">y</script></td>
				</tr>
				<tr><td>2.4</td><td>1.7</td></tr>
				<tr><td>2.8</td><td>1.85</td></tr>
				<tr><td>3.2</td><td>1.79</td></tr>
				<tr><td>3.6</td><td>1.95</td></tr>
				<tr><td>4.0</td><td>2.1</td></tr>
				<tr><td>4.2</td><td>2.0</td></tr>
				<tr><td>5.0</td><td>2.7</td></tr>
			</tbody>
		</table>
	</div>
	<div style="display:inline-block; vertical-align:middle;">
		<img src="/images/figures/lin_reg_scatter.png">
	</div>
</div>
{:/nomarkdown}

선형 회귀 분석의 목적은 이러한 점에 가장 적합한 1차 함수를 찾는 것입니다. 1차 함수의 일반 방정식은 $$ f(x) = m \cdot x + b $$이며, 여기서 $$m$$는 기울기이고 $$b$$는 y절편입니다. 따라서, 선형 회귀를 해결하는 것은 $$f(x)$$가 $$y$$와 최대한 비슷하도록 최상의 $$m$$과 $$b$$의 값을 결정하는 것입니다. 무작위로 몇 가지 후보들을 시험해 봅시다.

{% include figure_multi.md path1="/images/figures/lin_reg_randomtries.png" caption1="무작위로 선택된 1차 함수 후보 3가지" %}

분명히 처음 두 1차 함수들은 데이터에 잘 맞지 않습니다. 세 번째 것은 다른 두 개보다 조금 더 잘 맞는 것 같습니다. 하지만 어떻게 판단할 수 있을까요? 우리는 얼마나 적합한지를 표현할 수 있는 공식적인 방법이 필요하고, 바로 손실 함수를 정의해서 이를 표현할 수 있습니다.

## 손실 함수
손실 함수(또는 비용 함수라고도 함)는 선형 회귀 분석 중 데이터 세트에서 발생하는 오류의 양을 측정하는 것입니다. 다양한 손실 함수가 존재하지만, 모든 함수는 기본적으로 주어진 $$x$$에서 예측된 y 값과 실제 값 사이의 거리에 대해 불이익을 줍니다. 예를 들어, 위의 중간 예에서 나온 1차 함수를 보면 $$f(x) = -0.11 \cdot x + 2.5$$의 실제 값과 빨간색 점선으로 예측된 값 사이의 오차 한계를 주목합니다.

{% include figure_multi.md path1="/images/figures/lin_reg_error.png" caption1="" %}

매우 일반적인 손실 함수 중 평균 제곱 오차(MSE)가 있습니다. MSE를 계산하려면 모든 오류 막대를 선택하고 길이를 제곱한 다음 평균을 구하면 됩니다.

$$ MSE = \frac{1}{n} \sum_i{(y_i - f(x_i))^2} $$

$$ MSE = \frac{1}{n} \sum_i{(y_i - (mx_i + b))^2} $$

우리는 앞서 제안한 세 1차 함수 각각에 대한 MSE를 계산할 수 있습니다. 이렇게 하면 첫 번째 함수는 0.17의 MSE 값을 가지고 있고, 두 번째 함수는 0.08이며, 세 번째 함수는 0.02로 내려갑니다. 놀랄 것도 없이, 세 번째 함수는 MSE가 가장 낮아서 그것이 가장 적합하다는 우리의 추측을 확인시켜줍니다.

어떤 이웃한 1차 함수들에서 모든 $$m$$와 $$b$$에 대한 MSE를 계산하고 비교한다면 직관을 얻을 수 있습니다. 아래 그림을 생각해 보십시오. 이 그림은 기울기 $$m$$가 -2와 4 사이이고 절편 $$b$$가 -6과 8 사이인 범위에서 평균 제곱 오차를 두 가지 방법으로 시각화했습니다.

{% include figure_multi.md path1="/images/figures/lin_reg_mse.png" caption1="왼쪽: $ -2 \lem \le 4$ 및 $ -6 \le p \le 8 $에 대한 평균 제곱 오차를 표시한 그래프 <br/> 오른쪽: 동일 값을 표시한, 등고선도가 로그적으로 분포된 높이 횡단면이 있는 2차원 <ref=\"https://en.wikipedia.org/wiki/Contour_line\">등치선</a>" %}

위의 두 그래프를 보면, 우리의 MSE는 길쭉한 사발처럼 생겼다는 것을 알 수 있습니다. 이 사발은 이웃에 있는 대략 $$ (m,p) \approx (0.5, 1.0)$$의 타원형으로 평평하게 보입니다. 실제로 데이터 세트에 대한 선형 회귀 분석의 MSE를 그려보면 유사한 모양이 나타납니다. MSE를 최소화하기 위해 노력하고 있기 때문에 그릇에 담긴 가장 낮은 지점이 어디에 있는지 파악하는 것이 우리의 목표임을 알 수 있습니다.

## 더 많은 차원을 더하기

위의 예는 매우 단순한 것으로, 하나의 독립 변수인 $$x$$와 두 매개 변수 $$m$$와 $$b$$를 가지고 있습니다. 변수가 더 많으면 어떻게 될까요? 일반적으로 $$n$$개의 변수가 있는 경우, 변수의 1차 선형 함수는 다음과 같이 작성할 수 있습니다.

$$f(x) = b + w_1 \cdot x_1 + w_2 \cdot x_2 + ... + w_n \cdot x_n $$

행렬로 표시하자면, 이렇게 요약할 수 있습니다:

$$
f(x) = b + W^\top X
\;\;\;\;\;\;\;\;where\;\;\;\;\;\;\;\;
W = 
\begin{bmatrix}
w_1\\w_2\\\vdots\\w_n\\\end{bmatrix}
\;\;\;\;and\;\;\;\;
X = 
\begin{bmatrix}
x_1\\x_2\\\vdots\\x_n\\\end{bmatrix}
$$

이를 단순화하기 위해 사용할 수 있는 한 가지 방법은 편향 $b$를 단순히 다른 가중치로 생각하는 것입니다. 이 가중치는 항상 "더미" 입력 값 1에 곱해 표현됩니다. 즉, 다음과 같이 표현할 수 있습니다.

$$
f(x) = W^\top X
\;\;\;\;\;\;\;\;where\;\;\;\;\;\;\;\;
W = 
\begin{bmatrix}
b\\w_1\\w_2\\\vdots\\w_n\\\end{bmatrix}
\;\;\;\;and\;\;\;\;
X = 
\begin{bmatrix}
1\\x_1\\x_2\\\vdots\\x_n\\\end{bmatrix}
$$

이 등가 공식은 함수를 더 단순하게 $f(x) = W^\top X$로 표현할 수 있고, 편향을 가중치 중 하나로 생각하면, 매개 변수 하나만 최적화하면 되므로 아주 합리적인 방법입니다.

처음에는 차원이 늘어나는 것이 우리의 문제를 끔찍이 복잡하게 만드는 것처럼 보였지만, 알고 보니 문제의 공식은 차원이 2, 3 또는 어떤 수더라도 정확히 같습니다. 비록 지금 그것을 그리는 것은 불가능하지만, 어떤 차원에서는 그릇처럼 보이는 손실 함수가 존재합니다. -- 초-사발(hyper-bowl)이죠! 그리고 이전과 마찬가지로, 우리의 목표는 그 그릇의 가장 낮은 부분, 객관적으로 손실 함수가 특정 매개 변수와 데이터 세트에 대해 가질 수 있는 가장 작은 값을 찾는 것입니다.

그러면 어떻게 하면 맨 아래에 있는 그 지점이 정확히 어디에 있는지 계산할 수 있을까요? 가장 일반적인 방법은 분석적으로 해결하는 [최소 제곱법](https://ko.wikipedia.org/wiki/최소제곱법)이 있지만, 다양한 방법을 사용할 수 있습니다. 풀어야 할 매개변수가 하나 또는 두 개뿐일 때, 이것은 손으로 할 수 있고, 일반적으로 통계학이나 선형 대수학 입문 과정에서 배울 수 있습니다.

{% include further_reading.md title="Linear regression tutorial" author="Ozzie Liu" link="http://ozzieliu.com/2016/02/09/gradient-descent-tutorial/" %} 

{% include further_reading.md title="Implementation of linear regression in python" author="Chris Smith" link="https://crsmithdev.com/blog/ml-linear-regression/" %} 

{% include further_reading.md title="Artificial Neural Networks: Linear Regression (Part 1)" author="Brian Dolhansky" link="http://briandolhansky.com/blog/artificial-neural-networks-linear-regression-part-1" %} 

## 비선형성의 저주

그러나, 일반적인 최소 제곱은 신경망 최적화를 위해 사용될 수 없으므로, 위의 선형 회귀 분석을 해결하는 것은 여러분께 연습으로 남겨놓겠습니다. 선형 회귀를 사용할 수 없는 이유는 신경망은 비선형적이기 때문입니다. 우리가 제시한 선형 방정식과 신경망 사이의 본질적인 차이는 활성화 함수(예: 시그모이드, tanh, ReLU 또는 기타)의 존재입니다. 따라서, 위의 선형 방정식은 단순히 $y = b + W^\top X$인 반면, 시그모이드 활성화 함수를 갖는 단층 신경망은 $f(x) = \two(b + W^\top X)$가 될 것입니다.

이 비선형성은 매개 변수가 손실 함수의 모양에 영향을 미치는 데 서로 독립적으로 작용하지 않음을 의미합니다. 신경망의 손실 함수는 그릇 모양보다 더 복잡합니다. 울퉁불퉁하고 언덕과 수조가 가득합니다. "그릇 모양" 특징은 [볼록 함수](https://ko.wikipedia.org/wiki/볼록_함수)라고 하며, 다중 파라미터를 최적화할 때 매우 편리합니다. 볼록 손실 함수는 글로벌 최솟값(그릇의 바닥)을 보장하며 내리막길의 모든 도로가 볼록한 상태로 이어지도록 합니다.

그러나 비선형성을 도입함으로써, 우리는 신경망의 함수를 모델링하는 데 훨씬 더 많은 "유연성"을 제공하기 위해 이러한 편리성을 잃게 됩니다. 더 분석적으로 최솟값을 찾을 수 있는 쉬운 방법이 없다는 것입니다. 이 경우, 우리는 정답에 도달하기 위해 다단계의 수치적인 방법을 사용해야 합니다. 몇 가지 대안적 접근법이 존재하지만, 경사 하강법은 여전히 가장 대중적이고 효과적입니다. 다음 절에서는 어떻게 작동하는지 살펴보겠습니다.

# 경사 하강

우리가 다루어 온 일반적인 문제, 즉 어떤 객관적인 기능을 만족시키기 위한 매개 변수를 찾는 문제는 기계 학습에만 국한되지 않습니다. 실제로 그것은 오랫동안 알려진 [수학적 최적화](https://ko.wikipedia.org/wiki/수학적_최적화)에서 발견되는 매우 일반적인 문제이며, 단순한 신경망보다 훨씬 더 많은 시나리오에서 발견되었습니다. 오늘날, 신경망 학습을 포함한 다양한 다변수 함수 최적화 문제는 무작위 추측보다 훨씬 빠르고, 선형 회귀보다 더 강력한 알고리즘인 경사 하강법을 사용합니다.

## 경사 하강법

직관적으로, 경사 하강법이 작동하는 방식은 우리가 챕터 앞부분에서 제시한 산악인의 비유와 비슷합니다. 먼저, 매개 변수를 랜덤하게 추측하는 것으로 시작합니다. 그런 다음 매개 변수 변경과 관련하여 손실 함수가 가장 아래로 기울어지는 방향을 파악하고 해당 방향으로 약간 이동합니다. 다시 말하면 손실 함수가 가장 큰 폭으로 감소하도록 모든 매개 변수를 조정할 양을 결정합니다. 우리는 우리가 가장 낮은 점을 발견했다고 만족할 때까지 이 과정을 계속해서 반복합니다.

손실 함수가 가장 아래로 기울어지는 방향을 파악하려면 모든 매개 변수에 대해 손실 함수의 [기울기](https://ko.wikipedia.org/wiki/기울기)를 계산해야 합니다. 기울기(gradient)는 [미분](https://ko.wikipedia.org/wiki/미분)의 다차원 일반화입니다. 이는 각 변수에 대한 함수의 편미분을 포함하는 벡터입니다. 즉, 모든 축에 대한 손실 함수의 기울기를 포함한 벡터입니다.

선형 회귀를 해결하는 가장 편리한 방법은 일반 최소 제곱법이나 다른 단일 단계 방법을 사용하는 것이라고 이미 언급했지만, 선형 회귀 분석을 위한 경사 하강법을 사용하는 간단한 예를 보기 위해 선형 회귀 분석으로 다시 돌아가 보겠습니다.

이전 섹션에서 소개한 평균 제곱 오차 손실을 다시 생각해봅시다. 이를 $J$라고 합니다.

$$ J = \frac{1}{n} \sum_i{(y_i - (mx_i + b))^2} $$

최적화하려는 매개 변수는 $m$와 $b$입니다. 각각에 대해 $J$의 편미분을 계산해 보겠습니다.

$$ \frac{\partial J}{\partial m} = \frac{2}{n} \sum_i{x_i \cdot (y_i - (mx_i + b))} $$

$$ \frac{\partial J}{\partial b} = \frac{2}{n} \sum_i{(y_i - (mx_i + b))} $$

그 방향으로 얼마나 더 나아가야 할까요? 이것은 아주 중요한 사항입니다. 그리고 보통의 경사 하강법에서는, 이것은 수동으로 결정하는 하이퍼 파라미터(hyperparameter)로 남겨집니다. 학습률로 알려진 이 하이퍼 파라미터는 일반적으로 가장 중요하고 민감한 하이퍼 파라미터이며 종종 $$\alpha$$로 표시됩니다. $$\alpha$$가 너무 낮게 설정되어 있으면 가장 낮은 곳으로 이동하는 데 참을 수 없을 정도로 오랜 시간이 걸릴 수 있습니다. $$\alpha$$가 너무 높으면 올바른 경로에서 오버 슈팅(overshoot)되거나 심지어는 위로 올라갈 수도 있습니다.

할당 작업을 $:=$로 나타내면 두 매개 변수에 대한 업데이트 단계를 다음과 같이 작성할 수 있습니다.

$$ m := m - \alpha \cdot \frac{\partial J}{\partial m} $$

$$ b := b - \alpha \cdot \frac{\partial J}{\partial b} $$

위에서 설명한 간단한 선형 회귀 분석을 해결하기 위해서 이 방법을 사용하면 다음과 같은 결과를 얻을 수 있습니다.

{% include figure_multi.md path1="/images/figures/lin_reg_mse_gradientdescent.png" caption1="두 개의 매개 변수를 사용한 선형 회귀 분석에 대한 경사 하강 예입니다. 매개 변수를 랜덤하게 추측하고 손실 함수의 맨 아래에 도달할 때까지 기울기 방향으로 조금씩 이동해 반복적으로 위치를 업데이트합니다." %}

그리고 만약 더 많은 차원이 있다면요? 모든 매개 변수를 $w_i$로 표시하면,
$f(x) = b + W^\top X$로 표시할 수 있습니다. 그런 다음 위의 예를 다차원 사례에 대해 추론할 수 있습니다. 기울기 표기법을 사용하여 보다 간결하게 표현할 수 있습니다. $\nabla J$로 표현하는 $J$의 기울기는 각각의 편미분을 포함하는 벡터임을 잊지 마세요. 따라서 위의 업데이트 단계를 다음과 같이 나타낼 수 있습니다.

$$ \nabla J(W) = \Biggl(\frac{\partial J}{\partial w_1}, \frac{\partial J}{\partial w_2}, \cdots, \frac{\partial J}{\partial w_N} \Biggr) $$

$$ W := W - \alpha \nabla J(W) $$

위의 공식은 일반적인 경사 하강에 대한 표준 공식입니다. 선형 회귀 분석 또는 모든 현실적인 선형 최적화 문제에 대한 최적의 매개 변수 집합을 얻을 수 있습니다. 만약 여러분이 이 공식의 중요성을 이해한다면, 여러분은 신경망이 어떻게 학습되는지 "간단히" 이해할 것입니다. 하지만 실제로는 신경망의 학습 과정을 복잡하게 만드는 어떤 것들이 있고, 다음 절에서 그것들을 다루는 방법에 대해 다루도록 하겠습니다.

# 신경망에 경사 하강법 적용하기

## 볼록하다는 문제

이전 섹션에서는 단순한 선형 회귀 문제에 대해 경사 하강을 실행하는 방법을 보여 주었고, 그렇게 하면 올바른 매개 변수를 찾을 수 있다고 선언했습니다. 이것은 우리가 했던 것처럼 선형 모델을 최적화하는 것은 사실이지만, 활성화 함수가 가지고 있는 비선형성 때문에 신경망에는 적용되지 않습니다. 결과적으로, 신경망의 손실 함수는 '그릇 모양'이 아니고, 볼록하지 않습니다. 대신, 수많은 언덕과 계곡, 곡선 및 기타 불규칙성으로 인해 손실 함수가 훨씬 더 복잡합니다. 즉, 손실값이 주변에서는 가장 낮지만 반드시 절대 최소값(또는 "global minima")은 아닌 "극솟값(local minima)"이 많다는 의미입니다. 이것은 우리가 경사 하강을 할 때, 우리는 우연히 극솟값에 갇힐 수 있다는 것을 의미합니다.

{% include figure_multi.md path1="/images/figures/non_convex_function.png" caption1="두 개의 매개 변수가 있는 볼록하지 않은 손실 함수면의 예입니다. 심층 신경망에서는 수백만 개의 매개 변수를 다루고 있지만, 기본 원리는 그대로입니다. 출처: <a href=\"http://videolectures.net/site/normal_dl/tag=983679/deeplearning2015_bengio_theoretical_motivations_01.pdf\">Yoshua Bengio</a>." %}

이 책의 범위를 벗어난 어떤 이론적 이유로 인해 이것은 딥 러닝에서 큰 문제가 아닌 것으로 밝혀졌습니다. 왜냐하면 다른 기준들과 함께 충분한 숨겨진 단위가 있을 때, 대부분의 극솟값은 합리적으로 절대 최곳값에 가까워 "충분히" 좋기 때문입니다. [Dauphin et al](https://arxiv.org/abs/1406.2572)에 따르면, 극솟값보다 더 큰 문제는 기울기가 0에 매우 근접하는 [안장점](https://ko.wikipedia.org/wiki/안장점)입니다. 이것이 사실인 이유에 대한 설명은 [요슈아 벤지오](http://www.iro.umontreal.ca/~bengio/yoshua_en/)의 [강의](http://videolectures.net/deeplearning2015_bengio_theoretical_motivations/)를 참고하세요(28절, 1:09:41)

극솟값이 큰 문제가 아니라는 사실에도 불구하고, 우리는 여전히 그것들이 전혀 문제가 되지 않도록 극복하는 것을 선호합니다. 한 가지 방법은 경사 하강이 작동하는 방식을 수정하는 것입니다. 다음 절에서는 이 방법을 설명합니다.

## 확률적, 배치 그리고 미니 배치 경사 하강

극솟값 외에도, "기본" 경사 하강은 또 다른 큰 문제가 있습니다: 너무 느립니다. 신경망은 수억 개의 매개 변수를 가질 수 있습니다. 즉, 데이터 세트의 단일 예제를 평가하려면 수억 개의 작업이 필요합니다. 게다가 데이터 세트의 모든 지점에서 평가된 경사 하강("batch gradient downlation")은 매우 비싸고 느린 작업입니다. 더욱이, 모든 데이터 세트에는 고유의 중복성이 있기 때문에, 어쨌든 점들의 충분히 많은 부분 집합에서 전체의 경사를 예상할 수 있어, 기울기를 추정하기 위한 부분 경사 하강에 불필요한 비용이 많이 든다는 것을 알 수 있습니다.

우리는 [확률적 경사 하강법(SGD)](https://en.wikipedia.org/wiki/Stochastic_gradient_descent)이라는 수정된 경사 하강법을 사용하여 이 문제와 극솟값 문제를 모두 해결할 수 있습니다. SGD를 사용하여 데이터 세트를 섞은 다음 각 샘플을 개별적으로 살펴보고, 단일 점에 대한 기울기를 계산하고, 각 샘플에 대한 가중치 업데이트를 수행합니다. 단일 예제가 특이치일 수 있고 실제 기울기의 근사치가 반드시 좋은 것은 아니기 때문에 처음에는 좋지 않은 생각처럼 보일 수 있습니다. 그러나 데이터 세트의 각 샘플에 대해 임의의 순서로 이 작업을 수행하면 전체 기울기 업데이트 경로의 변동이 평균화되고 좋은 해로 수렴됩니다. 게다가 SGD는 업데이트를 더 "덜컹거리고" 불규칙하게 만들어 극솟값과 안장점을 벗어나 계곡의 바닥에 갇히지 않도록 도와줍니다.

SGD는 특히 손실 함수면이 불규칙한 경우에 유용합니다. 그러나 일반적으로는 전체 데이터 세트가 각각 $$K$$개의 샘플을 갖도록 동일한 크기로 나누어진 $$N$$개의 미니 배치를 사용하는 미니 배치 경사 하강법(MB-GD)을 사용하는 것이 일반적인 접근 방식이다. $$K$$는 적은 양의 양수이거나 수십 또는 수백이 될 수도 있습니다. 특정 아키텍처와 애플리케이션에 따라 다릅니다. $$K=1$$이면 SGD이고 $$K$$가 전체 데이터 세트의 크기라면 배치 경사 하강입니다. 혼란스럽게도, 때때로 사람들은 MB-GD와 한 번에 하나의 표본을 모두 참조하기 위해 "SGD"라고 말합니다.

MB-GD를 사용하면 다음 두 가지 모두를 최대한 활용할 수 있습니다. 기울기는 SGD보다 부드럽고 안정적이며 전체 기울기와 상당히 유사할 뿐 아니라, 각 업데이트에 대해 데이터 세트의 모든 샘플을 평가할 필요가 없기 때문에 굉장히 빠릅니다. MB-GD는 병렬 가능한 행렬 연산으로 인해 매우 효율적으로 계산됩니다.

{% include figure_multi.md path1="/images/figures/bumpy_gradient_descent.png" caption1="볼록하지 않은 손실 함수에 대한 경사 하강법 예시(신경망과 같은), $\theta_0$ 와 $\theta_1$, 2개의 매개 변수가 있습니다. Source: <a href=\"http://www.holehouse.org/mlclass/01_02_Introduction_regression_analysis_and_gr.html\">Andrew Ng</a>." %}

실제로 MB-GD와 SGD는 신경망의 손실 함수를 효율적으로 최적화하는 데 효과적입니다. 하지만, 그들은 약점도 가지고 있습니다.

 - 앞서 언급한 안장점의 문제. 손실 함수 기울기가 0에 매우 근접하는 매개변수에 갇힐 수 있습니다.
 - 학습률은 수동으로 설정해야 하는 하이퍼 파라미터로 유지되며, 이는 어려운 작업입니다. 학습률이 너무 작으면 수렴 속도가 느려지고, 너무 크면 올바른 경로를 벗어나 오버슈팅할 수 있습니다.

## 모멘텀(Momentum)

[모멘텀](https://distill.pub/2017/momentum/)은 가중치 업데이트가 관성을 갖는 변형된 경사 하강법을 가리킵니다. 즉, 가중치 업데이트는 이제는 현재에서 점진적으로 바뀌는 함수가 아니라 이전 업데이트의 속도에 따라 점차 조정됩니다.

표준 경사 하강에서 기울기 $$\nabla J(W)$$를 계산하고 학습률 $\alpha$와 함께 다음 매개 변수 업데이트 공식을 사용한다는 점을 기억합시다.

$$ W_{t} := W_{t} - \alpha \nabla J(W_{t}) $$

이전에 생략된 현재 단계를 나타내기 위해 첨자$$t$$를 추가했습니다. 이와는 대조적으로, 모멘텀이 있는 경사 하강에 대한 일반적인 공식은 다음과 같습니다.

$$ z_{t} := \beta z_{t-1} + \nabla J(W_{t-1}) $$

$$ W_{t} := W_{t-1} - \alpha z_{t} $$

매개 변수 업데이트에서 우리는 기울기 $$\nabla J(W_{t})$$를 이전 단계의 기울기를 고려한 더 복잡한 함수 $$z_{t}$$로 교체했습니다. $$\beta$$가 높게 설정될수록 매개 변수 업데이트의 모멘텀이 커집니다. $$\diples = 0$$를 설정하면 공식이 일반적인 경사 하강으로 되돌아갑니다. $$\alpha$$는 이전과 같이 프로세스의 전체 학습 속도를 제어합니다.

업데이트 경로가 볼이 아래로 굴러가는 것과 같다고 생각할 수 있습니다. 기울기가 크게 변화하는 지역에 도달하더라도, 기울기의 경로를 따라 점차 변화할 뿐 자신의 관성을 따라 거의 동일한 방향으로 계속 진행될 것입니다. 모멘텀은 이전 업데이트에서 축적된 속도를 통해 안장점과 극솟값을 벗어날 수 있도록 도와줍니다. 또한 경사도가 다른 방향이 아닌, 일부 방향을 따라 강하게 기울어지는 국지적으로 불규칙한 손실 함수 표면을 따라 일어날 수 있는 급격한 방향 전환을 막는 데 도움이 됩니다.

기존 모멘텀 공식의 한 가지 대안으로는 다음과 같은 네스테로프 가속 경사 하강(Nesterov accelerated gradient descent)이 있습니다.

$$ z_{t} := \beta z_{t-1} + \nabla J(W_{t-1} - \beta z_{t-1} ) $$

유일한 변화는 현재 ($$W_{t-1}$$)인 기울기를 평가하는 대신, 그 방향으로 이동하는 모멘텀이 축적될 경우, 다음 단계 ($$W_{t-1} - \betaz_{t-1}$$)에서 우리가 있을 위치를 대략 평가하는 것입니다. 현재 위치 대신 해당 지점에서 기울기를 계산하면 앞으로의 손실 함수 표면을 더 잘 예측하고 그에 따라 모멘텀 항을 조정할 수 있습니다. 아래 그림으로 보여드립니다.

{% include figure_multi.md path1="/images/figures/nesterov_acceleration.jpg" caption1="네스테로프 모멘텀은 기울기 항을 계산하기 위해 다음 업데이트에 있을 대략적인 위치를 \"미리 봅니다\". 출처: <a href=\"https://cs231n.github.io/neural-networks-3/\">스탠퍼드 CS231n</a>." %}

모멘텀 방법은 매우 잘 작동하지만, MB-GD 및 SGD와 마찬가지로 매개 변수 간의 비대칭에도 불구하고 전체에서 단 하나의 공식을 사용합니다. 이와는 대조적으로, 그라데이션의 각 요소에 적용하는 방법에는 몇 가지 장점이 있습니다. 다음 절에서 살펴보겠습니다. [distill.pub](https://distill.pub)의 기사에서는 모멘텀을 수학적으로 설명보고 모멘텀이 작동하는 이유를 알기 쉽게 설명하고 있습니다.

{% include further_reading.md title="Why momentum works" author="Gabriel Goh" link="https://distill.pub/2017/momentum/" %} 

## 적용 기법

모멘텀은 여러 종류가 있습니다. 일반적으로 경사 하강 시 매개 변수를 업데이트하기 위한 빠르고, 효율적이며, 정확한 전략을 찾는 것이 이 분야의 과학 연구의 핵심 목표이지만, 이에 대한 논의는 이 책의 범위를 벗어납니다. 대신 이 절에서는 실제 구현에서 잘 볼 수 있는 몇 가지 변형 방법들을 간략하게 정리해 보겠습니다. 더욱 포괄적인 설명은 다른 온라인 자료를 참고하시기 바랍니다.

학습 과정에서 가장 큰 골칫거리 중 하나는 학습률 $$\alpha$$를 설정하는 것입니다. 일반적으로 초기 $$\alpha$$는 처음에 설정되며, 일부 시간 단계에 걸쳐 점진적으로 감소하도록 내버려두어 더 정확하게 좋은 해로 수렴하도록 합니다. $$\alpha$$는 각 개별 매개 변수에 모두 동일하게 적용됩니다.

하지만 여전히 만족스럽지 않습니다. 각 단계에서 손실 함수 표면의 특성과 관계없이 개별 매개 변수에 대해 동일하게 설정된 학습률을 따라야 한다고 가정하기 때문입니다. 또한, 학습률 $$\alpha$$와 감소율을 설정하는 방법은 처음부터 명확하지 않습니다. 모멘텀과 네스테로프 모멘텀은 업데이트 속도를 기본 경사 하강이라는 "일률적인" 접근 방식보다는 주변 지역을 어느 정도 관찰하게 하여 이러한 부담을 줄이는 데 도움이 됩니다. 그러나 $$\alpha$$ 설정과 매개 변수 간의 유연성이 문제로 여겨집니다.

모든 매개 변수에서 손실의 분산이 크다는 가정하에 학습률을 각 매개 변수에 개별적으로 적용하여 이러한 단점을 해결하는 여러 가지 방법이 있습니다. 가장 간단한 매개 변수별 업데이트 방법은 [AdaGrad](http://jmlr.org/papers/v12/duchi11a.html)("Adaptive subGradient")입니다. AdaGrad를 사용하면 각 파라미터가 자체 경사에 따라 개별적으로 업데이트되지만, 그레이디언트가 큰 파라미터와 작은 파라미터 사이의 학습 속도를 균등하게 하는 새로운 계수로 업데이트됩니다. AdaGrad는 다음 공식으로 정의됩니다(참고: 첨자 $$i$$는 이전과 같은 시간 단계가 아닌 가중치의 인덱스입니다, 혼동하지 않도록 주의하세요).

$$ w_{i} := w_{i} - \frac{\alpha}{\sqrt{G_{i}+\epsilon}} \frac{\partial J}{\partial w_{i}} $$

$$\sqrt{G_{i}+\epsilon}$$는 학습이 시작된 이후 각 단계에 대한 해당 매개 변수에 대한 그라데이션의 제곱합을 나타냅니다(예: $$\epsilon$$항은 0으로 나누는 것을 피하기 위한 충분히 작은 숫자 $$10^-8$$ 등을 사용합니다). 그 양에 따라 각 매개 변수에 대해 $$\alpha$$를 나누면, 우리는 그 시점까지 큰 그레이디언트를 누려온 매개 변수에 대한 학습률을 효과적으로 늦추고 반대로 경미하거나 희박한 그레이디언트를 가진 매개 변수에 대한 학습을 가속합니다.

AdaGrad는 대부분 초기 학습률 $$\alpha$$를 하이퍼 파라미터로 처리할 필요가 없지만, 여전히 문제가 있습니다. AdaGrad의 일반적인 문제는 시간이 지남에 따라 각 매개 변수에 대해 $$G_{i}$$가 누적되고 업데이트의 크기가 감소함에 따라 학습이 조기에 중지될 수 있다는 것입니다. AdaGrad의 변형인 [AdaDelta](https://arxiv.org/abs/1212.5701),은 그레이디언트 누적 범위를 최신 업데이트에 제한하여 이를 효과적으로 해결합니다. AdaDelta와 매우 유사한 또 다른 방법으로는 [RMSprop](http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf)이 있습니다. RMSprop은 [Geoffrey Hinton](http://www.cs.toronto.edu/~hinton/)가 Coursera 수업 중에 제안했지만, 논문으로 출판되지는 않았습니다. 이 기술은 표준 [dupper](http://easings.net) 공식을 사용해 감쇠율을 이용하는 것으로, 더 간단한 방법으로 업데이트를 확인할 수 있습니다. 여기서 감쇠율은 하이퍼 파라미터가 됩니다. 따라서 AdaDelta와 RMSprop의 업데이트는 학습이 끝날 때까지 단조 감소 하는 대신 매개 변수와 시간에 따라 변하게 됩니다.

## Adam 및 업데이트 방법 비교

이 장에서 마지막으로 언급할 가치가 있는 방법이자 가장 최근에 제안된 방법 중 하나는 [Adam](http://arxiv.org/abs/1412.6980)이며, Adam의 이름은 적응형 모멘트 추정에서 파생된 이름입니다. Adam은 우리에게 적응법과 운동량 기반 방법 사이의 장점을 모두 제공합니다. AdaDelta 및 RMSprop와 마찬가지로 Adam은 과거 그레이디언트의 슬라이딩 윈도우에 따라 각 매개 변수의 학습율을 조정하지만, 시간 단계에 따라 경로를 매끄럽게 하는 모멘텀 구성 요소를 갖추고 있습니다.

여전히 더 많은 방법이 존재하고, 그것들에 대한 완전한 논의는 이 장의 범위를 벗어났습니다. 파생 모델 및 실제 팁을 포함한 자세한 내용은 [Sebastian Ruder의 블로그 게시물](http://ruder.io/optimizing-gradient-descent/index.html)에서 확인할 수 있습니다.

[Alex Radford](https://twitter.com/alecrad)의 멋진 시각화를 통해 지금까지 논의된 여러 그레이디언트 업데이트 방법 중 특징적인 동작을 보여줍니다. 운동량 기반 방법인 모멘텀 및 네스테로프 가속 경사 하강(NAG)은 "하강으로 너무 빠르게" 최적 경로를 오버슈팅하는 경향이 있고, 표준 SGD는 올바른 경로에서 너무 느리게 이동합니다. AdaGrad, AdaDelta, 그리고 RMSProp 같은 적응적 방법은 (Adam을 여기에 추가할 수도 있습니다) 매개 변수별로 유연성이 있어 두 가지 함정을 모두 피할 수 있습니다.

{% include figure_multi.md path1="/images/figures/opt2a.gif" caption1="좋은 매개 변수로 수렴되는 그레이디언트 업데이트 방법의 등고선도. 출처 <a href=\"https://www.twitter.com/alecrad\">Alec Radford</a>" path2="/images/figures/opt1a.gif" caption2="안장점에서 빠져나오는 그라데이션 업데이트 방법의 비교입니다. SGD가 끼이는 것을 주목하세요. 출처 <a href=\"https://www.twitter.com/alecrad\">Alec Radford</a>" %}

그렇다면 어떤 최적화 방법이 가장 효과적일까요? 이에 대한 간단한 해답은 없으며, 답변은 주로 데이터의 특성과 기타 교육 제약 및 고려 사항에 따라 달라집니다. Adam은 적어도 처음에는 유망한 방법으로 등장했습니다. 데이터가 희박하거나 불균일하게 분포된 경우에는 순수하게 적용된 방법이 가장 잘 작동하는 경향이 있습니다. 각 방법의 사용 시기에 대한 자세한 설명은 본 장의 범위를 벗어나기 때문에, 더 많은 정보는 최적화에 대한 학술 논문이나 [요슈아 벤지오](https://arxiv.org/pdf/1206.5533v2.pdf)와 같은 실용적인 요약을 참고하시기 바랍니다.

경사 하강 최적화에 대한 자세한 내용은 다음을 참조하십시오.

{% include further_reading.md title="An overview of gradient descent optimization algorithms" author="Sebastian Ruder" link="http://ruder.io/optimizing-gradient-descent/index.html" %} 

{% include further_reading.md title="Optimizing convolutional networks (CS231n)" author="Andrej Karpathy" link="https://cs231n.github.io/neural-networks-3/" %} 


# 하이퍼 파라미터 및 평가

이제 네트워크의 매개 변수를 최적화하는 개념을 이해했으므로 전체 절차를 요약할 준비가 되었습니다. 최종 모델을 훈련하는 쉬운 방법은 전체 데이터에 대해 경사 하강 절차를 실행하는 것입니다. 하지만 이렇게 하면 문제가 발생합니다. 모델의 정확성을 어떻게 평가할 수 있을까요? 레이블링된 데이터를 모두 교육에 사용했기 때문에, 레이블을 평가하는 유일한 방법은 교육 세트에서 모델을 다시 실행하고 출력과 "Ground Truth(실측 정보)"(주어진 레이블) 간의 차이를 측정하는 것입니다. 이것이 왜 나쁜 관행인지 이해하기 위해서는 과적합(overfitting) 현상을 이해하는 것이 필요합니다.

## 과적합(Overfitting)

[과적합](https://en.wikipedia.org/wiki/Overfitting)은 알 수 없는 데이터(처음 학습 목표)로 일반화하는 대신 교육 세트를 정확하게 예측하기 위해 모델이 지나치게 최적화되어 있는 상황을 설명합니다. 이러한 현상은 모델이 교육 세트에 완벽하게 부합해 학습 데이터 세트에 포함된 노이즈까지 포착하기 때문에 발생할 수 있습니다.

우리가 과적합을 생각할 수 있는 한 가지 방법은 우리의 알고리즘이 일종의 \"부정행위\"를 배웠다는 것입니다. 알려진 데이터에 대해 최소한의 오류만 얻을 수 있도록 방향을 맞춤으로써 인위적으로 높은 점수를 얻도록 설득하는 것입니다. 마치 패션이 어떻게 작동하는지 학습하려고 할 때, 70년대 디스코 나이트클럽의 사람들의 사진만 보고 모든 패션이 나팔바지, 청재킷, 통굽 신발이 전부라고 생각하는 것과 같습니다. 여러분의 가까운 친구나 가족 구성원 중에도 비슷한 사람이 있을지도 모르겠네요.

이러한 예는 아래 그래프에서 확인할 수 있습니다. 우리는 11개의 데이터 포인트를 블랙으로 부여받았고, 2개의 함수는 그것에 맞도록 학습되었습니다. 하나는 데이터를 대략 캡처하는 직선입니다. 다른 하나는 곡선 형태로, 오류 없이 데이터에 완벽하게 일치합니다. 후자는 교육 데이터에 대한 오류(실제로 0입니다)가 적기 때문에 처음에는 더 잘 맞는 것처럼 보입니다. 그러나 기본 분포를 잘 따르지 않는 미지의 점에 대해서는 성능이 저하될 수 있습니다.

{% include figure_multi.md path1="/images/figures/overfitting.png" caption1="과적합의 예시. 직선은 간단하며 약간의 오류와 함께 데이터 지점을 대략 파악합니다. 곡선의 오차는 0이지만 매우 복잡하고 일반화되지 않을 가능성이 높습니다. 출처: <a href=\"https://en.wikipedia.org/wiki/Overfitting\">Wikipedia</a>." %}

어떻게 하면 과적합을 피할 수 있을까요? 가장 간단한 해결책은 데이터 세트를 학습 세트와 테스트 세트로 나누는 것입니다. 학습 세트는 위에서 설명한 최적화 절차에 사용되지만, 테스트 세트는 학습된 모델에 사용되고 모델의 정확도를 평가합니다. 테스트 세트는 학습에서 제외되기 때문에 모델이 "부정행위", 즉 나중에 퀴즈로 낼 샘플을 암기하는 것을 방지할 수 있습니다. 학습 중에 우리는 학습 세트와 테스트 세트에서 모델의 정확도를 모니터링할 수 있습니다. 학습 시간이 길어질수록 학습 정확도가 점점 더 높아지지만, 어느 시점부터 테스트 세트의 정확도는 더 이상 개선되지 않을 가능성이 높습니다. 이것은 학습을 중단하라는 신호입니다. 일반적으로 학습 정확도가 테스트 정확도보다 높을 것으로 예상하지만, 예상보다 훨씬 높으면 과적합이 일어나고 있다는 것을 뜻합니다.

## 교차 검증 및 하이퍼 파라미터 선택

위의 절차는 과적합을 해결하기 위한 좋은 시작이지만, 여전히 충분하지 않습니다. 최적화를 시작하기 전에 해야 할 중요한 결정들이 많이 남아 있습니다. 어떤 모델 아키텍처를 사용해야 할까요? 레이어와 숨겨진 유닛은 몇 개나 있어야 할까요? 학습률 및 기타 하이퍼 파라미터는 어떻게 설정해야 할까요? 여러 가지 설정을 시도해보고 테스트 세트에서 성능이 가장 좋은 것으로 결정할 수 있습니다. 그러나 문제는 하이퍼 파라미터가 임의적이거나 알려지지 않은 테스트 집합이 아니라 특정 테스트 집합만 최적화하는 값으로 설정될 위험이 있다는 것입니다. 이것은 또다시 과적합 되었다는 것을 의미할 것입니다.

이에 대한 해결책은 두 세트가 아닌 세 세트, 즉, 학습 세트, 검증 세트 및 테스트 세트로 데이터를 분할하는 것입니다. 일반적으로 학습 세트가 전체 데이터의 70%에서 80%를 차지하고 테스트와 검증 세트가 나머지를 균등하게 나눕니다. 이제 학습 세트에 대해 학습하고 검증 세트에 대해 평가하여 최적의 하이퍼 파라미터를 찾고 학습을 중단해야 하는 시기를 파악합니다(일반적으로 검증 세트 정확도가 향상되지 않을 때). 때로는 교차 검증이 선호됩니다. 이러한 유형에서는 학습 및 검증 세트가 동일한 크기(예: 10)의 부분 집합으로 분할되고 각 부분 집합이 번갈아 검증 세트가 됩니다. 하나의 검증 세트가 지속해서 사용될 수도 있습니다. 확인 후 계속 남겨두고 테스트 세트를 사용하여 평가합니다.

최근, 많은 연구자는 훈련 과정 자체 내에서 아키텍처와 하이퍼 파라미터를 학습하는 방법을 고안하기 시작했습니다. [Google Brain](https://research.google.com/teams/brain/))의 연구원들은 이것을 [AutoML](https://research.googleblog.com/2017/05/using-machine-learning-to-explore.html)이라고 부릅니다. 이러한 방법은 여전히 인간의 개입이 필요한 기계 학습의 구성요소를 자동화하는 데 큰 잠재력을 가지고 있으며, 문제를 정의하고 데이터 세트를 제공하는 것만으로 기계 학습을 할 수 있는 미래를 보여줍니다.

## 정규화

[정규화](https://en.wikipedia.org/wiki/Regularization_(mathematics))는 과적합을 방지하거나 바람직하지 않은 속성을 억제하기 위해 신경 네트워크에 제약을 가하는 것을 말합니다. 과적합이 발생하는 경우 중 하나는 가중치의 크기가 너무 커질 때입니다. 위의 예에서 보았던 것처럼 네트워크 출력 함수의 모양이 학습 세트의 기본 노이즈마저 학습하는 것이 바로 그것입니다.

정규화하는 한 가지 방법은 큰 가중치에 불이익을 추가하여 목표 함수를 수정하는 것입니다. 신경망을 $$f$$하면 우리가 최적화하고 있는 손실 함수는 다음의 평균 제곱 오차로 표현됩니다.

$$ J = \frac{1}{n} \sum_i{(y_i - f(x_i))^2} $$

여기서 $R(f)$로 표시된 L2 정규화 항에 손실 함수를 추가해 큰 가중치에 불이익을 줄 수 있습니다.

$$ R(f) = \frac{1}{2} \lambda \sum{w^2} $$

이 항은 정규화 항의 전체 크기(즉, 영향)를 제어하는 새로운 하이퍼 파라미터 $\lambda$에 곱한 것입니다. $\frac{1}{2}$ 승수는 쉽게 미분하기 위해 사용됩니다. 이것을 기존 손실 함수에 추가하면 다음과 같습니다.

$$ J = \frac{1}{n} \sum_i{(y_i - f(x_i))^2} + R(f) $$

이 정규화 항은 경사 하강이 위에서 본 것 처럼 큰 가중치를 쌓아 급격하게 변하지 않는 매개 변수를 찾는 데 도움이 됩니다.

[L1-거리](https://en.wikipedia.org/wiki/Taxicab_geometry) 또는 "맨하탄 거리"와 같은 다른 정규화 방법이 존재합니다. 이들은 약간 다른 특징이 있지만, 효과는 거의 같습니다.

## 드롭아웃(Dropout)

드롭아웃은 정규화를 위한 성공적인 기술로, 2014년 [Nitish Srivastava et al](http://www.cs.toronto.edu/~rsalakhu/tfta/srivastava14a.pdf)에 의해 도입되었습니다. 학습 중에, 드롭아웃이 레이어에 적용될 때, 뉴런의 일부(일반적으로 하이퍼 파라미터의 20%~50%)는 연결과 함께 무작위로 비활성화됩니다. 어떤 뉴런이 탈락하는지는 학습 중에 끊임없이 무작위로 바뀝니다. 그 효과는 네트워크가 뉴런에 지나치게 의존하는 경향을 감소시키는 것입니다. 특정 뉴런을 항상 이용할 수는 없기 때문입니다. 이렇게 하면 네트워크가 보다 균형 잡힌 표현을 학습하고 과적합을 방지하는 데 도움이 됩니다. 원 논문의 그림은 아래와 같습니다.

{% include figure_multi.md path1="/images/figures/dropout.png" caption1="드롭아웃은 과적합 방지를 위한 방법으로서 학습 중에 일부 뉴런을 무작위로 비활성화합니다. 출처: <a href=\"http://www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf\">Srivastava et al</a>." %}

정규화를 위한 또 다른 이색적인 방법은 [입력 내용에 약간의 노이즈가 있는 경우](https://www.microsoft.com/en-us/research/publication/training-with-noise-is-equivalent-to-tikhonov-regularization/)입니다. 또 다른 방법들이 많이 제안되었지만, 여기서는 자세히 다루지 않을 것입니다.

# 오차역전파법(Backpropagation)

이 시점에서, 우리는 신경망의 매개 변수화를 위한 경사 하강법과 적용, 그리고 모멘텀 방법을 포함한 여러 가지 대안들을 소개했습니다. 어떤 방법을 선택하든, 네트워크의 가중치와 편향에 대한 손실 함수의 기울기를 계산해야 합니다. 이것은 쉬운 일이 아닙니다. 그 이유를 알기 위해 어떻게 해야 할지 생각해 보죠.

표준 경사 하강법에서 가중치 업데이트 공식은 다음과 같았습니다.

$$ W_{t} := W_{t} - \alpha \nabla J(W_{t}) $$

$\nabla J(W_{t})$는 손실의 그레이디언트이며, 우리가 조사한 모든 경사 하강법 변형에서 어떤 형태로든 계산되어야 합니다. 그라데이션은 각 매개 변수에 대한 손실 함수의 편미분으로 이루어진 벡터이며, 다음 ($t$는 간결성을 위해 생략됨)과 같습니다.

$$ \nabla J(W) = \Biggl(\frac{\partial J}{\partial w_1}, \frac{\partial J}{\partial w_2}, \cdots, \frac{\partial J}{\partial w_N} \Biggr) $$

각 $\frac{\partial J}{\partial w_i}$를 어떻게 계산할 수 있습니까? 이렇게 하는 가장 분명한 방법은 일반적인 미분방정식으로 계산하는 것입니다.

$$ \frac{\partial J}{\partial w_i} \approx \frac{J(W + \epsilon e_i) - J(W)}{\epsilon} $$

여기서 $e_i$는 원-핫 벡터(one-hot vector, 지수 $i$에서 1을 제외한 모든 0)이고 $\epsilon$은 매우 작은 숫자입니다. 이론적으로는 잘 되지만, 수렴 속도 측면에서 큰 문제가 있습니다. 그레이디언트의 단일 요소를 얻으려면 손실 함수를 $W + \epsilone_i$ 및 $W$에서 모두 계산해야 합니다. $W$의 경우 이 작업을 한 번만 수행하면 되지만, 모든 단일 가중치 $w_i$에 대해 $J(W + \epsilone_i)$가 필요합니다. 전형적인 심층 신경망은 수백만 혹은 심지어 수억의 가중치를 가지고 있습니다. 이 작업에는 수백만 번의 전진 패스를 수행해야 하며, 각 패스는 하나의 가중치 업데이트를 위해 수백만 번의 작업을 수행합니다. 이렇게 신경망을 학습시키는 것은 완전히 비현실적입니다.

어떻게 하면 될까요? 사실, 이것은 [오차역전파법](https://ko.wikipedia.org/wiki/%EC%97%AD%EC%A0%84%ED%8C%8C)이 개발되기 전까지 신경망 학습에 큰 걸림돌이었습니다. 역전파를 발명한 사람이 누구인가 하는 문제는 [논쟁적 이슈](https://plus.google.com/100849856540000067209/posts/9BDtGwCDL7D),)가 있습니다. 많은 사람이 역사 속에서 다양한 문제에 적용되는 동일한 개념을 발견한 것입니다. 신경망과 크게 관련되어 있지만, 오차 역전파(backprop)는 지속해서 미분 가능한 다변수 함수에 대한 그레이디언트를 계산하는 것과 관련된 모든 문제에 사용될 수 있습니다. 따라서 오차 역전파법의 발전은 어느 정도 신경망의 발전과 같이 이루어졌습니다. 2014년 [Jürgen Schmidhuber](http://www.idsia.ch/~jugeren)는 [오차역전파법 개발 배경 연구에 대한 리뷰](http://people.idsia.ch/~jugeren/who-juger-back propagation.differ)를 정리했습니다.

오차 역전파는 [David Rumelhart](https://en.wikipedia.org/wiki/David_Rumelhart), [Geoffrey Hinton](http://www.cs.toronto.edu/~hinton/) 및 [Ronald J. Williams](http://www.ccs.neu.edu/home/rjw/)가 1986년에 발표한 [획기적인 논문](https://www.iro.umontreal.ca/~hypt3395/backprop_old.pdf)에서 경사 하강에 의한 신경망 최적화를 위해 처음 적용되었습니다. 이후 80년대와 90년대에 [옌 르쿤](http://yann.lecun.com/ex/research/index.html)이 합성곱 신경망에 처음 적용했습니다. 신경망의 성공은 주로 그들의 팀의 노력에 의한 것이 큽니다.

오차역전파가 작동하는 방식에 대한 자세한 설명은 이 책의 범위를 벗어납니다. 대신, 이 단락은 오차 역전파가 무엇을  제공하는지 기본적인 관점을 설명할 것입니다. 기술적인 설명은 몇 가지 참고 문헌에 맡깁니다. 기본 아이디어는 역전파(backprop)를 사용하면 단순한 접근 방식을 사용할 때처럼 모든 매개 변수에 대해 하나의 전진 패스를 수행할 필요 없이 네트워크를 통과하는 단일 전진 및 후진 패스로 그레이디언트의 모든 요소를 계산할 수 있다는 것입니다. 이것은 미적분학의 [연쇄 법칙](https://ko.wikipedia.org/wiki/%EC%97%B0%EC%87%84_%EB%B2%95%EC%B9%99)을 활용함으로써 가능합니다. 연쇄 규칙을 사용하면 미분을 개별 함수 부분의 곱으로 분해 할 수 있습니다. 모든 연결을 따라 전진 패스의 차이를 추적하고 저장함으로써, 우리는 전진 패스의 끝에 있는 손실 항을 취하고 각 레이어를 통해 "오차를 거꾸로 전파"함으로써 기울기를 계산할 수 있습니다. 이렇게 하면 후진 패스는 전진 패스와 거의 동일한 양의 작업이 수행됩니다. 이는 학습 속도를 획기적으로 높이고 심층 신경망에서 경사 하강을 수행할 수 있도록 합니다.

오차 역전파법에 대한 자세한 설명은 다음 링크를 참조하십시오.

{% include further_reading.md title="How the backpropagation algorithm works" author="Michael Nielsen" link="http://neuralnetworksanddeeplearning.com/chap2.html" description="A free online book which introduces neural networks and deep learning from scratch" %} 

{% include further_reading.md title="Hacker's guide to Neural Networks" author="Andrej Karpathy" link="http://karpathy.github.io/neuralnets/" %} 

{% include further_reading.md title="Deep Learning Basics: Neural Networks and Stochastic Gradient Descent" author="Alex Minnaar" link="http://alexminnaar.com/deep-learning-basics-neural-networks-backpropagation-and-stochastic-gradient-descent.html" %} 

{% include further_reading.md title="[Video] Back Propagation Derivation for Feed Forward Artificial Neural Networks " author="Sully Chen" link="https://www.youtube.com/watch?v=gl3lfL-g5mA" %} 

{% include further_reading.md title="[Video] Neural network tutorial: the back-propagation algorithm (2 parts)" author="Ryan Harris" link="https://www.youtube.com/watch?v=aVId8KMsdUU" %} 

{% include further_reading.md title="Calculus on Computational Graphs: Backpropagation" author="Chris Olah" link="colah.github.io/posts/2015-08-Backprop/" %} 

{% include further_reading.md title="A Step by Step Backpropagation Example" author="Matt Mazur" link="https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/" %} 

# 하산하기

여기까지 읽었다면, 여러분은 지금쯤 이 챕터의 첫머리에 제시된 등산가의 비유의 의미를 파악하셨을 것입니다. 만약 그렇다면, 축하드립니다. 신경망 학습을 위한 실제 과학적 연구가 훨씬 더 쉽게 접근할 수 있도록 충분히 성공적인 것에 대한 예술과 과학에 대해 감사하게 생각합니다. 세월이 흐르면서, 많은 과학자는 오차 역전파를 위해 다양하고 색다른 확장을 제안했습니다. Geoffrey Hinton 자신을 포함한 다른 사람들은 기계 학습이 [역전파에서 벗어나 다시 시작해야 한다](http://www.i-programmer.info/news/105-artificial-intelligence/11135--geoffrey-hinton-says-ai-needs-to-start-over.html)고 제안했습니다. 그러나 이 책의 집필 시점에서는, 역전파를 통한 경사 하강법은 신경망과 대부분의 다른 기계 학습 모델을 학습하는 가장 유력한 패러다임으로, 가까운 미래에도 그럴 것으로 보입니다.

이 책의 다음 몇 장에서는 좀 더 발전된 주제를 살펴보기 시작할 것입니다. 우리는 다음 장에서 [합성곱 신경망](/ml4a/convnets/)과 특히 이 책의 핵심인 예술 및 기타 창작에 대한 다양한 응용을 소개할 것입니다.
